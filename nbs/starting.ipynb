{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, List, Dict, Tuple, Union\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drwho/anaconda3/lib/python3.9/site-packages/torch/onnx/_internal/_beartype.py:35: UserWarning: module 'beartype.roar' has no attribute 'BeartypeDecorHintPep585DeprecationWarning'\n",
      "  warnings.warn(f\"{e}\")\n",
      "/Users/drwho/anaconda3/lib/python3.9/site-packages/torch_geometric/graphgym/config.py:19: UserWarning: Could not define global config object. Please install 'yacs' via 'pip install yacs' in order to use GraphGym\n",
      "  warnings.warn(\"Could not define global config object. Please install \"\n"
     ]
    }
   ],
   "source": [
    "from helpers.utils import set_seed\n",
    "from helpers.encoders import PosEncoder\n",
    "from helpers.metrics import LossesAndMetrics\n",
    "from helpers.model import ModelType\n",
    "from helpers.classes import GumbelArgs, EnvArgs, ActionNetArgs, ActivationType\n",
    "from helpers.classes import Pool\n",
    "from helpers.dataset_classes.dataset import DatasetBySplit\n",
    "\n",
    "from models.CoGNN import CoGNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "dataset_string = 'roman_empire'\n",
    "act_dim = 16 \n",
    "env_dim = 64\n",
    "act_num_layers = 1\n",
    "env_num_layers = 3\n",
    "act_model_type = 'MEAN_GNN'\n",
    "env_model_type = 'MEAN_GNN'\n",
    "pos_enc = PosEncoder.NONE\n",
    "\n",
    "# Set the seed\n",
    "set_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset_classes.dataset import DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[22662, 300], edge_index=[2, 65854], y=[22662], train_mask=[22662, 10], val_mask=[22662, 10], test_mask=[22662, 10])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_dataset = DataSet.from_string(dataset_string)\n",
    "\n",
    "# download the dataset at this step\n",
    "dataset = pre_dataset.load(seed=seed, pos_enc=pos_enc)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things relate to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<MetricType.ACCURACY: 1>,\n",
       " 2,\n",
       " CrossEntropyLoss(),\n",
       " 18,\n",
       " <function helpers.dataset_classes.dataset.DataSet.gin_mlp_func.<locals>.mlp_func(in_channels: int, out_channels: int, bias: bool)>,\n",
       " <ActivationType.GELU: 2>,\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_type = pre_dataset.get_metric_type()\n",
    "decimal = pre_dataset.num_after_decimal()\n",
    "task_loss = metric_type.get_task_loss()\n",
    "out_dim = metric_type.get_out_dim(dataset=dataset)\n",
    "gin_mlp_func = pre_dataset.gin_mlp_func()\n",
    "env_act_type = pre_dataset.env_activation_type()\n",
    "folds = pre_dataset.get_folds(fold)\n",
    "metric_type, decimal, task_loss, out_dim, gin_mlp_func, env_act_type, folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GumbelArgs(learn_temp=False, temp_model_type=<ModelType.LIN: 3>, tau0=0.5, temp=0.01, gin_mlp_func=<function DataSet.gin_mlp_func.<locals>.mlp_func at 0x1041254c0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "learn_temp = False\n",
    "temp_model_type = ModelType.LIN\n",
    "tau0 = 0.5\n",
    "temp = 0.01 # temperature\n",
    "gin_mlp_func=gin_mlp_func\n",
    "\n",
    "gumbel_args = GumbelArgs(\n",
    "    learn_temp=learn_temp, temp_model_type=temp_model_type, tau0=tau0, temp=temp, gin_mlp_func=gin_mlp_func)\n",
    "gumbel_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 300\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "dropout = 0.2\n",
    "step_size = None\n",
    "gamma = None\n",
    "num_warmup_epochs = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment net arguements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvArgs(model_type=<ModelType.MEAN_GNN: 5>, num_layers=3, env_dim=64, layer_norm=False, skip=False, batch_norm=False, dropout=0.2, act_type=<ActivationType.GELU: 2>, dec_num_layers=1, pos_enc=<PosEncoder.NONE: 1>, dataset_encoders=<DataSetEncoders.NONE: 1>, metric_type=<MetricType.ACCURACY: 1>, in_dim=300, out_dim=18, gin_mlp_func=<function DataSet.gin_mlp_func.<locals>.mlp_func at 0x1041254c0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_model_type = ModelType.MEAN_GNN\n",
    "env_num_layers = env_num_layers\n",
    "env_dim = 64\n",
    "skip = False\n",
    "batch_norm = False\n",
    "layer_norm = False\n",
    "dec_num_layers = 1\n",
    "pos_enc = PosEncoder.NONE\n",
    "\n",
    "env_args = EnvArgs(\n",
    "    model_type=env_model_type, num_layers=env_num_layers,\n",
    "    env_dim=env_dim, skip=skip, batch_norm=batch_norm,\n",
    "    layer_norm=layer_norm, dec_num_layers=dec_num_layers,\n",
    "    pos_enc=pos_enc, metric_type=metric_type, in_dim=dataset[0].x.shape[1],\n",
    "    out_dim=out_dim, gin_mlp_func=gin_mlp_func,\n",
    "    dropout=dropout, act_type=env_act_type, dataset_encoders=pre_dataset.get_dataset_encoders())\n",
    "env_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AactionNet arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_args = ActionNetArgs(\n",
    "    model_type=ModelType.MEAN_GNN,\n",
    "    num_layers=act_num_layers,\n",
    "    hidden_dim=act_dim,\n",
    "    dropout=dropout,\n",
    "    act_type=ActivationType.RELU,\n",
    "    env_dim=env_dim,\n",
    "    gin_mlp_func=gin_mlp_func,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "edge_ratios_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoGNN(\n",
       "  (env_net): ModuleList(\n",
       "    (0): EncoderLinear(in_features=300, out_features=64, bias=True)\n",
       "    (1-3): 3 x WeightedGNNConv(64, 64)\n",
       "    (4): Linear(in_features=64, out_features=18, bias=True)\n",
       "  )\n",
       "  (hidden_layer_norm): Identity()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (in_act_net): ActionNet(\n",
       "    (net): ModuleList(\n",
       "      (0): WeightedGNNConv(64, 2)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (out_act_net): ActionNet(\n",
       "    (net): ModuleList(\n",
       "      (0): WeightedGNNConv(64, 2)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (pooling): BatchIdentity()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CoGNN(gumbel_args=gumbel_args, env_args=env_args, action_args=action_args, pool=Pool.NONE).to(\n",
    "            device=device)\n",
    "\n",
    "optimizer = pre_dataset.optimizer(model=model, lr=lr, weight_decay=0)\n",
    "\n",
    "scheduler = pre_dataset.scheduler(\n",
    "    optimizer=optimizer,\n",
    "    step_size=step_size,\n",
    "    gamma=gamma,\n",
    "    num_warmup_epochs=num_warmup_epochs,\n",
    "    max_epochs=max_epochs)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(dataset_by_split, model, optimizer, scheduler, pbar, num_fold: int):\n",
    "    train_loader = DataLoader(dataset_by_split.train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset_by_split.val, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset_by_split.test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_losses_n_metrics = metric_type.get_worst_losses_n_metrics()\n",
    "    for epoch in range(max_epochs):\n",
    "        train_func(train_loader=train_loader, model=model, optimizer=optimizer)\n",
    "        train_loss, train_metric, _ = test_func(\n",
    "            loader=train_loader, model=model, split_mask_name=\"train_mask\", calc_edge_ratio=False\n",
    "        )\n",
    "        if pre_dataset.is_expressivity():\n",
    "            val_loss, val_metric = train_loss, train_metric\n",
    "            test_loss, test_metric = train_loss, train_metric\n",
    "\n",
    "        else:\n",
    "            val_loss, val_metric, _ = test_func(\n",
    "                loader=val_loader, model=model, split_mask_name=\"val_mask\", calc_edge_ratio=False\n",
    "            )\n",
    "            test_loss, test_metric, _ = test_func(\n",
    "                loader=test_loader, model=model, split_mask_name=\"test_mask\", calc_edge_ratio=False\n",
    "            )\n",
    "        \n",
    "        losses_n_metrics = LossesAndMetrics(\n",
    "            train_loss=train_loss,\n",
    "            val_loss=val_loss,\n",
    "            test_loss=test_loss,\n",
    "            train_metric=train_metric,\n",
    "            val_metric=val_metric,\n",
    "            test_metric=test_metric,\n",
    "        )\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(losses_n_metrics.val_metric)\n",
    "\n",
    "        # best metrics\n",
    "        if metric_type.src_better_than_other(\n",
    "            src=losses_n_metrics.val_metric, other=best_losses_n_metrics.val_metric\n",
    "        ):\n",
    "            best_losses_n_metrics = losses_n_metrics\n",
    "        \n",
    "        log_str = f\"Split: {num_fold}, epoch: {epoch}\"\n",
    "        for name in losses_n_metrics._fields:\n",
    "            log_str += f\",{name}={round(getattr(losses_n_metrics, name), decimal)}\"\n",
    "        log_str += f\"({round(best_losses_n_metrics.test_metric, decimal)})\"\n",
    "        pbar.set_description(log_str)\n",
    "        pbar.update(n=1)\n",
    "\n",
    "    edge_ratios = None\n",
    "    if pre_dataset.not_synthetic():\n",
    "        _, _, edge_ratios = test_func(\n",
    "            loader=test_loader, model=model, split_mask_name=\"test_mask\", calc_edge_ratio=True\n",
    "        )\n",
    "    return best_losses_n_metrics, edge_ratios\n",
    "\n",
    "def train_func(train_loader, model, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        if batch_norm and (data.x.shape[0] == 1 or data.num_graphs == 1):\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        node_mask = pre_dataset.get_split_mask(\n",
    "            data=data, batch_size=data.num_graphs, split_mask_name=\"train_mask\"\n",
    "        ).to(device=device)\n",
    "        edge_attr = data.edge_attr\n",
    "        if data.edge_attr is not None:\n",
    "            edge_attr = edge_attr.to(device=device)\n",
    "        \n",
    "        scores, _ = model(\n",
    "            data.x.to(device=device),\n",
    "            edge_index=data.edge_index.to(device=device),\n",
    "            batch=data.batch.to(device=device),\n",
    "            edge_attr=edge_attr,\n",
    "            edge_ratio_node_mask=None,\n",
    "            pestat=pos_enc.get_pe(data=data, device=device),\n",
    "        )\n",
    "        train_loss = task_loss(scores[node_mask], data.y.to(device=device)[node_mask])\n",
    "\n",
    "        train_loss.backward()\n",
    "        if pre_dataset.clip_grad():\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "def test_func(loader, model, split_mask_name: str, calc_edge_ratio: bool) -> Tuple[float, Any, Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_metric, total_edge_ratios = 0, 0, 0\n",
    "    total_scores = np.empty(shape=(0, model.env_args.out_dim))\n",
    "    total_y = None\n",
    "    for data in loader:\n",
    "        if batch_norm and (data.x.shape[0] == 1 or data.num_graphs == 1):\n",
    "            continue\n",
    "        node_mask = pre_dataset.get_split_mask(\n",
    "            data=data, batch_size=data.num_graphs, split_mask_name=split_mask_name\n",
    "        ).to(device=device)\n",
    "        if calc_edge_ratio:\n",
    "            edge_ratio_node_mask = pre_dataset.get_edge_ratio_node_mask(\n",
    "                data=data, split_mask_name=split_mask_name\n",
    "            ).to(device=device)\n",
    "        else:\n",
    "            edge_ratio_node_mask = None\n",
    "        edge_attr = data.edge_attr\n",
    "        if data.edge_attr is not None:\n",
    "            edge_attr = edge_attr.to(device=device)\n",
    "        \n",
    "        scores, edge_ratios = model(\n",
    "            data.x.to(device=device),\n",
    "            edge_index=data.edge_index.to(device=device),\n",
    "            edge_attr=edge_attr,\n",
    "            batch=data.batch.to(device=device),\n",
    "            edge_ratio_node_mask=edge_ratio_node_mask,\n",
    "            pestat=pos_enc.get_pe(data=data, device=device),\n",
    "        )\n",
    "\n",
    "        eval_loss = task_loss(scores, data.y.to(device=device))\n",
    "\n",
    "        total_scores = np.concatenate((total_scores, scores[node_mask].detach().cpu().numpy()))\n",
    "        if total_y is None:\n",
    "            total_y = data.y.to(device=device)[node_mask].detach().cpu().numpy()\n",
    "        else:\n",
    "            total_y = np.concatenate((total_y, data.y.to(device=device)[node_mask].detach().cpu().numpy()))\n",
    "        \n",
    "        total_loss += eval_loss.item() * data.num_graphs\n",
    "        total_edge_ratios += edge_ratios * data.num_graphs\n",
    "    \n",
    "    metric = metric_type.apply_metric(scores=total_scores, target=total_y)\n",
    "\n",
    "    loss = total_loss / len(loader.dataset)\n",
    "    edge_ratios = total_edge_ratios / len(loader.dataset)\n",
    "    return loss, metric, edge_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_fold(\n",
    "    dataset_by_split, model, optimizer, scheduler, num_fold: int):\n",
    "    with tqdm.tqdm(total=max_epochs, file=sys.stdout) as pbar:\n",
    "        best_losses_n_metrics, edge_ratios = train_and_test(\n",
    "            dataset_by_split=dataset_by_split,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            pbar=pbar,\n",
    "            num_fold=num_fold,\n",
    "        )\n",
    "    return best_losses_n_metrics, edge_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_by_split = pre_dataset.select_fold_and_split(num_fold=2, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 2, epoch: 299,train_loss=0.88,val_loss=0.88,test_loss=0.88,train_metric=0.71,val_metric=0.69,test_metric=0.69(0.68): 100%|██████████| 300/300 [02:13<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "best_losses_n_metrics, edge_ratios = single_fold(\n",
    "        dataset_by_split=dataset_by_split,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_fold=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[22662, 300], edge_index=[2, 65854], y=[22662], train_mask=[22662], val_mask=[22662], test_mask=[22662], batch=[22662], ptr=[2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset_by_split.train, batch_size=batch_size, shuffle=True)\n",
    "sample_data = next(iter(train_loader))\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     1,  ..., 22661, 22661, 22661],\n",
       "        [    1,     2,     0,  ..., 22653, 22659, 22660]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22662, 300]), torch.Size([2, 65854]), torch.Size([22662]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.x.shape, sample_data.edge_index.shape, sample_data.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the hood\n",
    "> What's in the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22662, 300])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x_ = model.hidden_layer_norm(sample_data.x)\n",
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22662, 64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x_ = model.env_net[0](x_, pestat=pos_enc.get_pe(data=sample_data, device=device))\n",
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22662, 2]), torch.Size([22662, 2]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    in_logits = model.in_act_net(\n",
    "        x = x_, edge_index=sample_data.edge_index,\n",
    "        env_edge_attr=None, act_edge_attr=None)\n",
    "\n",
    "    out_logits = model.out_act_net(\n",
    "        x = x_, edge_index=sample_data.edge_index,\n",
    "        env_edge_attr=None, act_edge_attr=None)\n",
    "in_logits.shape, out_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         ...,\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]),\n",
       " tensor([[0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         ...,\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    in_probs = F.gumbel_softmax(logits=in_logits, tau=temp, hard=True)\n",
    "    out_probs = F.gumbel_softmax(logits=out_logits, tau=temp, hard=True)\n",
    "in_probs, out_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(1.))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_probs.sum(-1).max(), in_probs.sum(-1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,     0,     1,  ..., 22661, 22661, 22661]),\n",
       " tensor([    1,     2,     0,  ..., 22653, 22659, 22660]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, v = sample_data.edge_index\n",
    "u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    u, v = sample_data.edge_index\n",
    "    edge_in_probs = in_probs[:,0][u]\n",
    "    edge_out_probs = out_probs[:,0][v]\n",
    "    edge_weights = edge_in_probs * edge_out_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7359)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): EncoderLinear(in_features=300, out_features=64, bias=True)\n",
       "  (1-3): 3 x WeightedGNNConv(64, 64)\n",
       "  (4): Linear(in_features=64, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.env_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/10,train_loss=0.89,val_loss=0.89,test_loss=0.89,train_metric=0.7,val_metric=0.69,test_metric=0.68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_str = f\"Fold {0}/{len(folds)}\"\n",
    "for name in best_losses_n_metrics._fields:\n",
    "    print_str += f\",{name}={round(getattr(best_losses_n_metrics, name), decimal)}\"\n",
    "print(print_str)\n",
    "print()\n",
    "metrics_list.append(best_losses_n_metrics.get_fold_metrics())\n",
    "\n",
    "if edge_ratios is not None:\n",
    "    edge_ratios_list.append(edge_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LossesAndMetrics(train_loss=0.8871640563011169, val_loss=0.8867784142494202, test_loss=0.8861422538757324, train_metric=0.7033801078796387, val_metric=0.6907325387001038, test_metric=0.6849629282951355)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_losses_n_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/drwho/github/CoGNN/nbs/starting.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/drwho/github/CoGNN/nbs/starting.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
